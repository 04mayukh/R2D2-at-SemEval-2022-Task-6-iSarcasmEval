{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amJ-AHG-7JAF"
      },
      "outputs": [],
      "source": [
        "# 4.15.0\n",
        "!pip install transformers\n",
        "!pip install ekphrasis\n",
        "!pip install emoji\n",
        "!pip install sentencepiece\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS-AUdar7sIx"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizerFast, TFBertModel\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow import keras\n",
        "from collections import Counter\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "transformers.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fAohk4hg8A_D"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/train.En.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['tweet'] = df_train['tweet']. fillna('')"
      ],
      "metadata": {
        "id": "CseK90Q_WXV-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "kp3NlU_0WhDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df_train['sarcastic'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evFOIfcnQXhN",
        "outputId": "0ba4056f-ac5e-417c-9292-12425d5a234e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 2601, 1: 867})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4CX_hEY-Weh"
      },
      "outputs": [],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMT_f2zMMg50"
      },
      "source": [
        "<h6>TPU CONF</h6>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV3HCdsnMgXZ"
      },
      "outputs": [],
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlC0BbxzZBDx"
      },
      "outputs": [],
      "source": [
        "text = df_train['tweet']\n",
        "Y_sarcastic = df_train['sarcastic']\n",
        "Counter(Y_sarcastic)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = np.array(text)\n",
        "Y_sarcastic = np.array(Y_sarcastic)"
      ],
      "metadata": {
        "id": "4J5XXDm0Tjch"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZVWyVGya3Mt"
      },
      "source": [
        "<h6>Text Train Split</h6>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J7UMBcpeZx2T"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V2lGHNy5ZVQI"
      },
      "outputs": [],
      "source": [
        "text_train, text_val, Y_train_sarcastic, Y_val_sarcastic = train_test_split(text, Y_sarcastic, test_size=0.1, random_state=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = pd.Series(text_train)\n",
        "text_val = pd.Series(text_val)"
      ],
      "metadata": {
        "id": "a6hDMftoTr1x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP3jB7OIZVMg"
      },
      "outputs": [],
      "source": [
        "print(\"Train\")\n",
        "print(np.shape(text_train), type(text_train))\n",
        "print(np.shape(Y_train_sarcastic), type(Y_train_sarcastic))\n",
        "print(Counter(Y_train_sarcastic))\n",
        "print(\"Val\")\n",
        "print(np.shape(text_val))\n",
        "print(np.shape(Y_val_sarcastic))\n",
        "print(Counter(Y_val_sarcastic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixx5hs5Za2JT"
      },
      "source": [
        "<h6>Text pre-processing</h6>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "s0VuTFUiZVJS"
      },
      "outputs": [],
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuT4bqHJbB8l"
      },
      "outputs": [],
      "source": [
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=True,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Hvr1XOf-bB5E"
      },
      "outputs": [],
      "source": [
        "def print_text(texts,i,j):\n",
        "    for u in range(i,j):\n",
        "        print(texts[u])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SHN3YeybB2Z"
      },
      "outputs": [],
      "source": [
        "print_text(text_train,0,5)\n",
        "print(\"##############################################################################################################\")\n",
        "print_text(text_val,0,5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#removing website names\n",
        "def remove_website(text):\n",
        "    return \" \".join([word if re.search(\"r'https?://\\S+|www\\.\\S+'|((?i).com$|.co|.net)\",word)==None else \"\" for word in text.split(\" \") ])\n",
        "\n",
        "# Training set \n",
        "text_train = text_train.apply(lambda text: remove_website(text))\n",
        "print_text(text_train,0,5)\n",
        "\n",
        "print(\"**************************************************************************\")\n",
        "\n",
        "# Validation set \n",
        "text_val = text_val.apply(lambda text: remove_website(text))\n",
        "print_text(text_val,0,5)"
      ],
      "metadata": {
        "id": "tVW8-VZXVP4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gQ4cYQKubBzZ"
      },
      "outputs": [],
      "source": [
        "# Functions for chat word conversion\n",
        "f = open(\"slang.txt\", \"r\")\n",
        "chat_words_str = f.read()\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B45gKA2ZbBw3"
      },
      "outputs": [],
      "source": [
        "# Chat word conversion\n",
        "# Training set\n",
        "text_train = text_train.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(text_train,0,5)\n",
        "\n",
        "print(\"********************************************************************************\")\n",
        "\n",
        "# Validation set\n",
        "text_val = text_val.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(text_val,0,5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FUnction for removal of emoji\n",
        "import emoji\n",
        "\n",
        "def convert_emojis(text):\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(\"_|-\",\" \",text)\n",
        "    return text\n",
        "\n",
        "# Training set\n",
        "text_train = text_train.apply(lambda text: convert_emojis(text))\n",
        "print_text(text_train,0,5)\n",
        "\n",
        "print(\"**************************************************************************\")\n",
        "\n",
        "# Validation set\n",
        "text_val = text_val.apply(lambda text: convert_emojis(text))\n",
        "print_text(text_val,0,5)"
      ],
      "metadata": {
        "id": "WJCERAX0XX-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jnyAeJgLbBtp"
      },
      "outputs": [],
      "source": [
        "def ekphrasis_pipe(sentence):\n",
        "    cleaned_sentence = \" \".join(text_processor.pre_process_doc(sentence))\n",
        "    return cleaned_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3VSPXylbBrA"
      },
      "outputs": [],
      "source": [
        "# Training set\n",
        "text_train = text_train.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Training set completed.......\")\n",
        "#Validation set\n",
        "text_val = text_val.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Validation set completed.......\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObiKEw2ucqSr"
      },
      "outputs": [],
      "source": [
        "u = lambda text: len(text.split(\" \"))\n",
        "sentence_lengths = []\n",
        "for x in text_train:\n",
        "    sentence_lengths.append(u(x))\n",
        "print(sorted(sentence_lengths)[-200:])\n",
        "print(len(sentence_lengths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCqoO60Rcr1p"
      },
      "source": [
        "</h6>Text processing complete</h6>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast, TFRobertaModel, MPNetTokenizerFast, TFMPNetModel, ElectraTokenizerFast, TFElectraModel, XLNetTokenizerFast, TFXLNetModel, AlbertTokenizerFast, TFAlbertModel, DebertaTokenizer, TFDebertaModel"
      ],
      "metadata": {
        "id": "xj1Z21O6VL10"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "tokenizer = MPNetTokenizerFast.from_pretrained(\"microsoft/mpnet-base\")\n",
        "# tokenizer = ElectraTokenizerFast.from_pretrained('google/electra-base-discriminator')"
      ],
      "metadata": {
        "id": "ViSlaVByVXAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Pxcc2BxAs2Q"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(list(text_train), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(train_encodings[\"input_ids\"]))\n",
        "\n",
        "val_encodings = tokenizer(list(text_val), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(val_encodings[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "I9xzpXMmAvXO"
      },
      "outputs": [],
      "source": [
        "def task1(input_shape):\n",
        "    inputs = keras.Input(shape=input_shape, dtype='int32')\n",
        "    input_masks = keras.Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "    # Text\n",
        "    model = TFMPNetModel.from_pretrained('microsoft/mpnet-base')\n",
        "    layer = model.layers[0]\n",
        "    embeddings = layer([inputs, input_masks])[0]\n",
        "    features = embeddings[:, 0, :] # Not used only in bert,albert where [1] pooler output is used\n",
        "    # features = embeddings[:, -1] used for xlnet\n",
        "\n",
        "    X = keras.layers.Dense(64,activation='elu')(features)\n",
        "\n",
        "    X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "    X = keras.layers.Dense(1,activation='elu',kernel_regularizer=keras.regularizers.l2(0.01))(X)\n",
        "    \n",
        "    # Add a sigmoid activation\n",
        "    X = keras.layers.Activation('sigmoid')(X)    \n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = keras.Model(inputs=[inputs,input_masks], outputs=[X])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUbNvg2INVQo"
      },
      "outputs": [],
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "NI6tyaehNWah"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluationMetric(keras.callbacks.Callback):   \n",
        "    \n",
        "    def __init__(self, val_encodings, val_masks, Y_val):\n",
        "        super(EvaluationMetric, self).__init__()\n",
        "        self.val_encodings = val_encodings\n",
        "        self.val_masks = val_masks\n",
        "        self.Y_val = Y_val\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        print(\"\\nTraining...\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        print(\"\\nEvaluating...\")\n",
        "        val_prediction = self.model.predict([self.val_encodings, self.val_masks])\n",
        "        \n",
        "        pred = []\n",
        "        for i in range(0,len(self.Y_val)):\n",
        "            num = val_prediction[i]\n",
        "            if(num > 0.5):\n",
        "              num = 1\n",
        "            else:\n",
        "              num = 0\n",
        "            pred.append(num)\n",
        "        \n",
        "        print(classification_report(self.Y_val, pred, digits=3))\n",
        "        \n",
        "evaluation_metric = EvaluationMetric(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], Y_val_sarcastic)"
      ],
      "metadata": {
        "id": "HUMtSTlmGw3C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IEuSqSqK_oi"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  model = task1((70,))\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=4e-5)\n",
        "  loss_fun = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "  metric = [tf.metrics.BinaryAccuracy(), tf.metrics.Precision(), tf.metrics.Recall()]\n",
        "  model.compile(optimizer=optimizer, loss=loss_fun, metrics=metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA1Cu7CJNm41"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G11KmoRuOARu"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uRslZCZUkCCn"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(filepath='/content/sarcasm-1.{epoch:03d}.h5',\n",
        "                                 verbose = 0,\n",
        "                                 save_weights_only=True,\n",
        "                                 epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "sarcastic = compute_class_weight('balanced', classes=[0,1], y=Y_train_sarcastic)\n",
        "class_weights_sarcastic = {0: sarcastic[0], 1: sarcastic[1]}\n",
        "print(\"sarcastic\")\n",
        "print(class_weights_sarcastic)"
      ],
      "metadata": {
        "id": "5uRn7cYP8NU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x = [train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"]],\n",
        "    y = Y_train_sarcastic,\n",
        "    validation_data = ([val_encodings[\"input_ids\"],val_encodings[\"attention_mask\"]],Y_val_sarcastic),\n",
        "    callbacks = [evaluation_metric, checkpoint],\n",
        "    batch_size = 128,\n",
        "    shuffle=True,\n",
        "    epochs=10,\n",
        "    class_weight=class_weights_sarcastic\n",
        ")"
      ],
      "metadata": {
        "id": "0lrPVhxDR4ZE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xHB1W0OiODW1"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"/content/sarcasm-1.008.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h5>Task 1 test</h5>"
      ],
      "metadata": {
        "id": "cGQCXZ-eRDlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_a = pd.read_csv('/content/taskA.En.input.csv')\n",
        "df_test_a.head()"
      ],
      "metadata": {
        "id": "rQuJGmPyRDTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_a.info()"
      ],
      "metadata": {
        "id": "SaHk-IsbRDP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test_a = df_test_a['text']\n",
        "print(type(text_test_a))\n",
        "print(len(text_test_a))"
      ],
      "metadata": {
        "id": "YSFSthhoTZ9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test_a = text_test_a.apply(lambda text: remove_website(text))\n",
        "text_test_a = text_test_a.apply(lambda text: chat_words_conversion(text))\n",
        "text_test_a = text_test_a.apply(lambda text: convert_emojis(text))\n",
        "text_test_a = text_test_a.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Test set completed.......\")"
      ],
      "metadata": {
        "id": "H9f6QxgoRDM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings_a = tokenizer(list(text_test_a), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(test_encodings_a[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "yJ8IyCqTUJWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_a = model.predict([test_encodings_a[\"input_ids\"], test_encodings_a[\"attention_mask\"]])"
      ],
      "metadata": {
        "id": "NfmB2VxoSC4E"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_answer_a = np.array(np.round(pred_test_a[:,0])).astype(int)\n",
        "Counter(test_answer_a)"
      ],
      "metadata": {
        "id": "x-xFcL-tSC0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('answer.txt', 'w') as outf:\n",
        "  outf.write('task_a_en' + '\\n')\n",
        "  for i in range(0, len(test_answer_a)-1):\n",
        "    outf.write(str(test_answer_a[i]) + '\\n')\n",
        "  outf.write(str(test_answer_a[len(test_answer_a)-1]))"
      ],
      "metadata": {
        "id": "0I0BzX5xSCxh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zipfile.ZipFile('sarcasm_electra_a.zip', mode='w').write(\"answer.txt\")"
      ],
      "metadata": {
        "id": "gFdo38VTXXv4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h5>Task 2</h5>"
      ],
      "metadata": {
        "id": "4pq6OmV1bkwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_b = df_train[df_train['sarcastic'] == 1]"
      ],
      "metadata": {
        "id": "k7Jt8cTqbnrT"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_b.head(3)"
      ],
      "metadata": {
        "id": "-krHzq6OeLpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_b.info()"
      ],
      "metadata": {
        "id": "8ee4utXubnoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_b = np.array(df_train_b['tweet'])\n",
        "sarcasm = np.array(df_train_b['sarcasm'])\n",
        "irony = np.array(df_train_b['irony'])\n",
        "satire = np.array(df_train_b['satire'])\n",
        "under = np.array(df_train_b['understatement'])\n",
        "over = np.array(df_train_b['overstatement'])\n",
        "rhetorical = np.array(df_train_b['rhetorical_question'])"
      ],
      "metadata": {
        "id": "4Ob_Ej5Sbnky"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sarcasm\")\n",
        "print(Counter(sarcasm))\n",
        "print(\"Irony\")\n",
        "print(Counter(irony))\n",
        "print(\"Satire\")\n",
        "print(Counter(satire))\n",
        "print(\"Understatement\")\n",
        "print(Counter(under))\n",
        "print(\"Over Statement\")\n",
        "print(Counter(over))\n",
        "print(\"Rhetorical\")\n",
        "print(Counter(rhetorical))"
      ],
      "metadata": {
        "id": "6xVNSb-dbnhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(text_train_b, text_val_b,\n",
        " Y_sarcasm_train, Y_sarcasm_val,\n",
        " Y_irony_train, Y_irony_val,\n",
        " Y_satire_train, Y_satire_val,\n",
        " Y_under_train, Y_under_val,\n",
        " Y_over_train, Y_over_val,\n",
        " Y_rhetorical_train, Y_rhetorical_val) = train_test_split(text_b, sarcasm, irony, satire, under, over, rhetorical, test_size=0.05, random_state=3)"
      ],
      "metadata": {
        "id": "44jdKYz0q9sl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train\")\n",
        "print(Counter(Y_sarcasm_train), type(Y_sarcasm_train))\n",
        "print(Counter(Y_irony_train), type(Y_irony_train))\n",
        "print(Counter(Y_satire_train), type(Y_satire_train))\n",
        "print(Counter(Y_under_train), type(Y_under_train))\n",
        "print(Counter(Y_over_train), type(Y_over_train))\n",
        "print(Counter(Y_rhetorical_train), type(Y_rhetorical_train))"
      ],
      "metadata": {
        "id": "AggO2a6Kq9pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Val\")\n",
        "print(Counter(Y_sarcasm_val), type(Y_sarcasm_val))\n",
        "print(Counter(Y_irony_val), type(Y_irony_val))\n",
        "print(Counter(Y_satire_val), type(Y_satire_val))\n",
        "print(Counter(Y_under_val), type(Y_under_val))\n",
        "print(Counter(Y_over_val), type(Y_over_val))\n",
        "print(Counter(Y_rhetorical_val), type(Y_rhetorical_val))"
      ],
      "metadata": {
        "id": "CHqBXJlGq9mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train_b = pd.Series(text_train_b)\n",
        "text_val_b = pd.Series(text_val_b)\n",
        "\n",
        "text_train_b = text_train_b.apply(lambda text: remove_website(text))\n",
        "text_train_b = text_train_b.apply(lambda text: chat_words_conversion(text))\n",
        "text_train_b = text_train_b.apply(lambda text: convert_emojis(text))\n",
        "text_train_b = text_train_b.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Training set completed.......\")\n",
        "\n",
        "text_val_b = text_val_b.apply(lambda text: remove_website(text))\n",
        "text_val_b = text_val_b.apply(lambda text: chat_words_conversion(text))\n",
        "text_val_b = text_val_b.apply(lambda text: convert_emojis(text))\n",
        "text_val_b = text_val_b.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Val set completed.......\")"
      ],
      "metadata": {
        "id": "Ht4EGUyHbneg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings_b = tokenizer(list(text_train_b), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(train_encodings_b[\"input_ids\"]))\n",
        "\n",
        "val_encodings_b = tokenizer(list(text_val_b), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(val_encodings_b[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "qOMFWNgGqwwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def task_2(input_shape):\n",
        "    inputs = keras.Input(shape=input_shape, dtype='int32')\n",
        "    input_masks = keras.Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "    # Text\n",
        "    model = TFMPNetModel.from_pretrained('microsoft/mpnet-base')\n",
        "    layer = model.layers[0]\n",
        "    embeddings = layer([inputs, input_masks])[0]\n",
        "    features = embeddings[:, 0, :] # Not used only in bert,albert where [1] pooler output is used\n",
        "    # features = embeddings[:, -1] used for xlnet\n",
        "\n",
        "    sarcasm = keras.layers.Dense(64,activation='relu')(features)\n",
        "    sarcasm = keras.layers.BatchNormalization()(sarcasm)\n",
        "    sarcasm = keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.01))(sarcasm)\n",
        "\n",
        "    irony = keras.layers.Dense(64,activation='relu')(features)\n",
        "    irony = keras.layers.BatchNormalization()(irony)\n",
        "    irony = keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.01))(irony)\n",
        "\n",
        "    satire = keras.layers.Dense(64,activation='relu')(features)\n",
        "    satire = keras.layers.BatchNormalization()(satire)\n",
        "    satire = keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.01))(satire)\n",
        "\n",
        "    under = keras.layers.Dense(64,activation='relu')(features)\n",
        "    under = keras.layers.BatchNormalization()(under)\n",
        "    under = keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.01))(under)\n",
        "    \n",
        "    over = keras.layers.Dense(64,activation='relu')(features)\n",
        "    over = keras.layers.BatchNormalization()(over)\n",
        "    over = keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.01))(over)\n",
        "\n",
        "    rhetorical = keras.layers.Dense(64,activation='relu')(features)\n",
        "    rhetorical = keras.layers.BatchNormalization()(rhetorical)\n",
        "    rhetorical = keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.01))(rhetorical)\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = keras.Model(inputs=[inputs,input_masks], outputs=[sarcasm, irony, satire, under, over, rhetorical])\n",
        "    return model"
      ],
      "metadata": {
        "id": "J21ekHLyqwsv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "3MhSSDh6qwps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluationMetricB(keras.callbacks.Callback):   \n",
        "    \n",
        "    def __init__(self, val_encodings, val_masks, Y_val):\n",
        "        super(EvaluationMetricB, self).__init__()\n",
        "        self.val_encodings = val_encodings\n",
        "        self.val_masks = val_masks\n",
        "        self.Y_val = Y_val\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        print(\"\\nTraining...\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        print(\"\\nEvaluating...\")\n",
        "        val_prediction = self.model.predict([self.val_encodings, self.val_masks])\n",
        "        \n",
        "        pred = np.round(val_prediction)\n",
        "\n",
        "        from sklearn.metrics import classification_report\n",
        "\n",
        "        print(\"Sarcasm\")\n",
        "        sarcasm = classification_report(self.Y_val[:,0], pred[0][:,0], digits=3, output_dict=True)\n",
        "        print(sarcasm['1.0'])\n",
        "        print(sarcasm['macro avg'])\n",
        "        print(\"##################################################################\")\n",
        "        print(\"Irony\")\n",
        "        irony = classification_report(self.Y_val[:,1], pred[1][:,0], digits=3, output_dict=True)\n",
        "        print(irony['1.0'])\n",
        "        print(irony['macro avg'])\n",
        "        print(\"##################################################################\")\n",
        "        print(\"Satire\")\n",
        "        satire = classification_report(self.Y_val[:,2], pred[2][:,0], digits=3, output_dict=True)\n",
        "        print(satire['1.0'])\n",
        "        print(satire['macro avg'])\n",
        "        print(\"##################################################################\")\n",
        "        print(\"Under statement\")\n",
        "        under = classification_report(self.Y_val[:,3], pred[3][:,0], digits=3, output_dict=True)\n",
        "        print(under['1.0'])\n",
        "        print(under['macro avg'])\n",
        "        print(\"##################################################################\")\n",
        "        print(\"Over statement\")\n",
        "        over = classification_report(self.Y_val[:,4], pred[4][:,0], digits=3, output_dict=True)\n",
        "        print(over['1.0'])\n",
        "        print(over['macro avg'])\n",
        "        print(\"##################################################################\")\n",
        "        print(\"Rhetorical\")\n",
        "        rhetorical = classification_report(self.Y_val[:,5], pred[5][:,0], digits=3, output_dict=True)\n",
        "        print(rhetorical['1.0'])\n",
        "        print(rhetorical['macro avg'])\n",
        "        print(\"##################################################################\")\n",
        "\n",
        "Y_val_b = np.column_stack((Y_sarcasm_val, Y_irony_val, Y_satire_val, Y_under_val, Y_over_val, Y_rhetorical_val))        \n",
        "evaluation_metric_b = EvaluationMetricB(val_encodings_b[\"input_ids\"], val_encodings_b[\"attention_mask\"], Y_val_b)"
      ],
      "metadata": {
        "id": "AgAO_9TKqwml"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import Loss\n",
        "from tensorflow.keras import backend as K\n",
        "class weightedBinaryCrossEntropy(Loss):\n",
        "\n",
        "  def __init__(self, weights):\n",
        "    super().__init__()\n",
        "    self.zero = weights[0]\n",
        "    self.one = weights[1]\n",
        "  \n",
        "  def call(self, y_true, y_pred):\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "    loss = tf.keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
        "    weight_vector = (y_true*self.one) + ((1-y_true)*self.zero)\n",
        "    loss = loss*weight_vector\n",
        "    if(len(loss) != 0):\n",
        "      return tf.keras.backend.mean(loss)\n",
        "    else:\n",
        "      return 0.0"
      ],
      "metadata": {
        "id": "YAc9qZcw61qe"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights(Y_train, name):\n",
        "  weights = compute_class_weight('balanced', classes=[0,1], y=Y_train)\n",
        "  class_weights = {0: weights[0], 1: weights[1]}\n",
        "  print(name)\n",
        "  print(class_weights)\n",
        "  return class_weights"
      ],
      "metadata": {
        "id": "Rr0YtI0j7Yf8"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_sarcasm = get_weights(Y_sarcasm_train, \"sarcasm\")\n",
        "class_weights_irony = get_weights(Y_irony_train, \"irony\")\n",
        "class_weights_satire = get_weights(Y_satire_train, \"satire\")\n",
        "class_weights_under = get_weights(Y_under_train, \"understatement\")\n",
        "class_weights_over = get_weights(Y_over_train, \"overstatement\")\n",
        "class_weights_rhetorical = get_weights(Y_rhetorical_train, \"rhetorical\")"
      ],
      "metadata": {
        "id": "_PS6S2__7qiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_loss = [weightedBinaryCrossEntropy(class_weights_sarcasm),\n",
        "                 weightedBinaryCrossEntropy(class_weights_irony),\n",
        "                 weightedBinaryCrossEntropy(class_weights_satire),\n",
        "                 weightedBinaryCrossEntropy(class_weights_under),\n",
        "                 weightedBinaryCrossEntropy(class_weights_over),\n",
        "                 weightedBinaryCrossEntropy(class_weights_rhetorical)\n",
        "                 ]"
      ],
      "metadata": {
        "id": "k_rwsIMS63aA"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  model_b = task_2((70,))\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=4e-5)\n",
        "  loss_fun = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "  metric = [tf.metrics.BinaryAccuracy(), tf.metrics.Precision(), tf.metrics.Recall()]\n",
        "  model_b.compile(optimizer=optimizer, loss=weighted_loss, metrics=metric)"
      ],
      "metadata": {
        "id": "eG5ut3KMaDCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_b.summary()"
      ],
      "metadata": {
        "id": "-l1c4oa96Mwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model_b)"
      ],
      "metadata": {
        "id": "1JXdPvXK6Mt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_b = ModelCheckpoint(filepath='/content/sarcasm-2.{epoch:03d}.h5',\n",
        "                                 verbose = 0,\n",
        "                                 save_weights_only=True,\n",
        "                                 epoch=1)"
      ],
      "metadata": {
        "id": "BUwezE7X6Mst"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# electra\n",
        "history = model_b.fit(\n",
        "    x = [train_encodings_b[\"input_ids\"], train_encodings_b[\"attention_mask\"]],\n",
        "    y = [Y_sarcasm_train, Y_irony_train, Y_satire_train, Y_under_train, Y_over_train, Y_rhetorical_train],\n",
        "    callbacks = [evaluation_metric_b, checkpoint_b],\n",
        "    batch_size = 128,\n",
        "    shuffle=True,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "o4RPcCRBdR-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_b.load_weights('/content/sarcasm-2.010.h5')"
      ],
      "metadata": {
        "id": "l_vEPIbtA0t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h7>Test B</h6>"
      ],
      "metadata": {
        "id": "S0VcvyKq9X9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_b = pd.read_csv('/content/taskB.En.input.csv')\n",
        "df_test_b.head()"
      ],
      "metadata": {
        "id": "Xha4ag2c8vYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_b.info()"
      ],
      "metadata": {
        "id": "jYcSKzV48vUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test_b = df_test_b['text']\n",
        "print(type(text_test_b))\n",
        "print(len(text_test_b))"
      ],
      "metadata": {
        "id": "U-qq4PeV8vRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test_b = text_test_b.apply(lambda text: remove_website(text))\n",
        "text_test_b = text_test_b.apply(lambda text: chat_words_conversion(text))\n",
        "text_test_b = text_test_b.apply(lambda text: convert_emojis(text))\n",
        "text_test_b = text_test_b.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Test set completed.......\")"
      ],
      "metadata": {
        "id": "GVxxQ7Br8vO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings_b = tokenizer(list(text_test_b), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(test_encodings_b[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "I91mYYaq8vLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_b = model_b.predict([test_encodings_b[\"input_ids\"], test_encodings_b[\"attention_mask\"]])"
      ],
      "metadata": {
        "id": "TlXFCyxm8vIo"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_list(answer):\n",
        "  print(np.shape(answer))\n",
        "  final = (np.round(answer)).astype(np.int)\n",
        "  print(\"Sarcasm\")\n",
        "  print(Counter(final[0][:,0]))\n",
        "  print(\"Irony\")\n",
        "  print(Counter(final[1][:,0]))\n",
        "  print(\"Satire\")\n",
        "  print(Counter(final[2][:,0]))\n",
        "  print(\"Understatement\")\n",
        "  print(Counter(final[3][:,0]))\n",
        "  print(\"Overstatement\")\n",
        "  print(Counter(final[4][:,0]))\n",
        "  print(\"Rhetorical\")\n",
        "  print(Counter(final[5][:,0]))\n",
        "  return final"
      ],
      "metadata": {
        "id": "F8DJSxB38u5J"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_answer = get_answer_list(pred_test_b)"
      ],
      "metadata": {
        "id": "y9tMe90G9tD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(final_answer)"
      ],
      "metadata": {
        "id": "nAThvn0VCVL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('answer.txt', 'w') as outf:\n",
        "  outf.write('sarcasm,irony,satire,understatement,overstatement,rhetorical_question\\n')\n",
        "  for i in range(0, len(test_encodings_b['input_ids'])):\n",
        "    outf.write(str(final_answer[0][i,0]) + ',' + str(final_answer[1][i,0]) + ',' + str(final_answer[2][i,0]) + ',' + str(final_answer[3][i,0]) + ',' + str(final_answer[4][i,0]) + ',' + str(final_answer[5][i,0]) + '\\n')"
      ],
      "metadata": {
        "id": "2PLDe0i5BoJQ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zipfile.ZipFile('sarcasm_electra_b.zip', mode='w').write(\"answer.txt\")"
      ],
      "metadata": {
        "id": "bT6IRcdABn8r"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h6>Task C</h6>"
      ],
      "metadata": {
        "id": "obgbLYKCFNNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_c = df_train[df_train['sarcastic'] == 1]"
      ],
      "metadata": {
        "id": "OtkPme5dFQnQ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_c.info()"
      ],
      "metadata": {
        "id": "Qlh3DeNlFQji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_c.head(2)"
      ],
      "metadata": {
        "id": "fPkd1nEOFQga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_c = np.array(df_train_c['tweet'])\n",
        "rephrase_c = np.array(df_train_c['rephrase'])"
      ],
      "metadata": {
        "id": "PItWTdyIFQdZ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_sarcastic_c = np.ones((len(text_c,)), dtype='int32')\n",
        "Y_rephrase_c = np.zeros((len(text_c,)), dtype='int32')\n",
        "print(np.shape(Y_sarcastic_c))\n",
        "print(np.shape(Y_rephrase_c))\n",
        "print(Counter(Y_sarcastic_c))\n",
        "print(Counter(Y_rephrase_c))"
      ],
      "metadata": {
        "id": "cq2TZv7sFQZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,400):\n",
        "  temp1 = text_c[i]\n",
        "  temp2 = rephrase_c[i]\n",
        "  text_c[i] = temp2\n",
        "  rephrase_c[i] = temp1\n",
        "  Y_sarcastic_c[i] = 0\n",
        "  Y_rephrase_c[i] = 1"
      ],
      "metadata": {
        "id": "zLmwixbAnw-0"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_c = pd.Series(text_c)\n",
        "rephrase_c = pd.Series(rephrase_c)"
      ],
      "metadata": {
        "id": "1zuccSDLnwyw"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train_c, text_val_c, rephrase_train_c, rephrase_val_c, Y_c_train, Y_c_val, Y_rephrase_train, Y_rephrase_val = train_test_split(text_c, rephrase_c, Y_sarcastic_c, Y_rephrase_c, test_size=0.1, random_state=3)"
      ],
      "metadata": {
        "id": "8rdDLE7aFQW2"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(Y_c_train))\n",
        "print(Counter(Y_rephrase_train))\n",
        "\n",
        "print(Counter(Y_c_val))\n",
        "print(Counter(Y_rephrase_val))"
      ],
      "metadata": {
        "id": "HdPuPE4MAGiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train\")\n",
        "print(len(text_train_c), type(text_train_c))\n",
        "print(len(rephrase_train_c), type(rephrase_train_c))\n",
        "print(len(Y_c_train), type(Y_c_train))\n",
        "print(len(Y_rephrase_train), type(Y_rephrase_train))\n",
        "\n",
        "print(\"Val\")\n",
        "print(len(text_val_c), type(text_val_c))\n",
        "print(len(rephrase_val_c), type(rephrase_val_c))\n",
        "print(len(Y_c_val), type(Y_c_val))\n",
        "print(len(Y_rephrase_val), type(Y_rephrase_val))"
      ],
      "metadata": {
        "id": "UydEPJ0bFQT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sarcastic\")\n",
        "text_train_c = text_train_c.apply(lambda text: remove_website(text))\n",
        "text_train_c = text_train_c.apply(lambda text: chat_words_conversion(text))\n",
        "text_train_c = text_train_c.apply(lambda text: convert_emojis(text))\n",
        "text_train_c = text_train_c.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Rephrase\")\n",
        "rephrase_train_c = rephrase_train_c.apply(lambda text: remove_website(text))\n",
        "rephrase_train_c = rephrase_train_c.apply(lambda text: chat_words_conversion(text))\n",
        "rephrase_train_c = rephrase_train_c.apply(lambda text: convert_emojis(text))\n",
        "rephrase_train_c = rephrase_train_c.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Train set completed.......\")"
      ],
      "metadata": {
        "id": "sfcZ-Oi7FQRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sarcastic\")\n",
        "text_val_c = text_val_c.apply(lambda text: remove_website(text))\n",
        "text_val_c = text_val_c.apply(lambda text: chat_words_conversion(text))\n",
        "text_val_c = text_val_c.apply(lambda text: convert_emojis(text))\n",
        "text_val_c = text_val_c.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Rephrase\")\n",
        "rephrase_val_c = rephrase_val_c.apply(lambda text: remove_website(text))\n",
        "rephrase_val_c = rephrase_val_c.apply(lambda text: chat_words_conversion(text))\n",
        "rephrase_val_c = rephrase_val_c.apply(lambda text: convert_emojis(text))\n",
        "rephrase_val_c = rephrase_val_c.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Test set completed.......\")"
      ],
      "metadata": {
        "id": "TL7FC5syFQOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings_c = tokenizer(list(text_train_c), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(train_encodings_c[\"input_ids\"]))\n",
        "\n",
        "val_encodings_c = tokenizer(list(text_val_c), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(val_encodings_c[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "XQrOjaSrFQLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings_rc = tokenizer(list(rephrase_train_c), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(train_encodings_rc[\"input_ids\"]))\n",
        "\n",
        "val_encodings_rc = tokenizer(list(rephrase_val_c), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(val_encodings_rc[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "qpKcKfegFQIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
        "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))"
      ],
      "metadata": {
        "id": "VbeH0VILLsCT"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(input_shape):\n",
        "  inputs = keras.Input(shape=input_shape, dtype='int32')\n",
        "  input_masks = keras.Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "  inputs_1 = keras.Input(shape=input_shape, dtype='int32')\n",
        "  input_masks_1 = keras.Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "  inputs_2 = keras.Input(shape=input_shape, dtype='int32')\n",
        "  input_masks_2 = keras.Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "  model = TFMPNetModel.from_pretrained('microsoft/mpnet-base')\n",
        "  layer = model.layers[0]\n",
        "  embeddings = layer([inputs, input_masks])[0]\n",
        "  features = embeddings[:, 0, :]\n",
        "  features = keras.layers.Dense(64,activation='elu')(features)\n",
        "  siamese_ = keras.Model([inputs, input_masks], features)\n",
        "\n",
        "  tower_1 = siamese_([inputs_1, input_masks_1])\n",
        "  tower_2 = siamese_([inputs_2, input_masks_2])\n",
        "\n",
        "  # Tower 1 outputs classifier\n",
        "  x_1 = keras.layers.Dense(64,activation='elu')(tower_1)\n",
        "  x_1 = keras.layers.BatchNormalization()(x_1)\n",
        "  x_1 = keras.layers.Dense(1,activation='sigmoid')(x_1)\n",
        "\n",
        "  # Tower 2 outputs classifier\n",
        "  x_2 = keras.layers.Dense(64,activation='elu')(tower_2)\n",
        "  x_2 = keras.layers.BatchNormalization()(x_2)\n",
        "  x_2 = keras.layers.Dense(1,activation='sigmoid')(x_2)\n",
        "\n",
        "  merge_layer = keras.layers.Lambda(euclidean_distance)([tower_1, tower_2])\n",
        "  normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
        "  output_layer = tf.keras.layers.Dense(1, activation=\"relu\")(normal_layer)\n",
        "  output_layer = tf.keras.layers.Activation('sigmoid')(output_layer)\n",
        "\n",
        "  siamese = keras.Model(inputs=[inputs_1, input_masks_1, inputs_2, input_masks_2], outputs=[x_1, x_2, output_layer])\n",
        "\n",
        "  return siamese"
      ],
      "metadata": {
        "id": "cgVt00y2K_aN"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(margin=1):\n",
        "    def contrastive_loss(y_true, y_pred):\n",
        "      margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
        "      loss = tf.math.reduce_mean((y_true) * margin_square)\n",
        "      if(tf.math.is_nan(loss)):\n",
        "        return 0.0\n",
        "      else:\n",
        "        return loss\n",
        "    return contrastive_loss"
      ],
      "metadata": {
        "id": "MCGdyS-8iWko"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "bvcns1uGekTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  model_c = get_model((70,))\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=4e-5)\n",
        "  loss_func = [tf.keras.losses.BinaryCrossentropy(from_logits=False), tf.keras.losses.BinaryCrossentropy(from_logits=False), loss()]\n",
        "  metric = [tf.metrics.BinaryAccuracy(), tf.metrics.Precision(), tf.metrics.Recall()]\n",
        "  model_c.compile(optimizer=optimizer, loss=loss_func, metrics=metric)"
      ],
      "metadata": {
        "id": "nXb12GUSN3Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_c.summary()"
      ],
      "metadata": {
        "id": "qcXbbNJrN3JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model_c)"
      ],
      "metadata": {
        "id": "f9oIqYgKN3GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluationMetricC(keras.callbacks.Callback):   \n",
        "    \n",
        "    def __init__(self, val_encodings, val_masks, rephrase_encodings, rephrase_masks, Y_val):\n",
        "        super(EvaluationMetricC, self).__init__()\n",
        "        self.val_encodings = val_encodings\n",
        "        self.val_masks = val_masks\n",
        "        self.rephrase_encodings = rephrase_encodings\n",
        "        self.rephrase_masks = rephrase_masks\n",
        "        self.Y_val = Y_val\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        print(\"\\nTraining...\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        print(\"\\nEvaluating...\")\n",
        "        val_prediction = self.model.predict([self.val_encodings, self.val_masks, self.rephrase_encodings, self.rephrase_masks])\n",
        "        \n",
        "        pred = np.round(val_prediction)\n",
        "\n",
        "        from sklearn.metrics import classification_report\n",
        "\n",
        "        print(\"Sarcasm\") # 1\n",
        "        print(classification_report(self.Y_val[:,0], pred[0][:,0], digits=3))\n",
        "        print(\"##################################################################\")\n",
        "\n",
        "        print(\"Rephrase\") # 0\n",
        "        print(classification_report(self.Y_val[:,1], pred[1][:,0], digits=3))\n",
        "        print(\"##################################################################\")\n",
        "      \n",
        "\n",
        "Y_val_c = np.column_stack((Y_c_val, Y_rephrase_val))        \n",
        "evaluation_metric_c = EvaluationMetricC(val_encodings_c[\"input_ids\"], val_encodings_c[\"attention_mask\"], val_encodings_rc[\"input_ids\"], val_encodings_rc[\"attention_mask\"], Y_val_c)"
      ],
      "metadata": {
        "id": "_cmBlE1oWFXW"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_c = ModelCheckpoint(filepath='/content/sarcasm-3.{epoch:03d}.h5',\n",
        "                                 verbose = 0,\n",
        "                                 save_weights_only=True,\n",
        "                                 epoch=1)"
      ],
      "metadata": {
        "id": "gBlMJKO8WFTr"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_c.fit(\n",
        "    x = [train_encodings_c[\"input_ids\"], train_encodings_c[\"attention_mask\"], train_encodings_rc[\"input_ids\"], train_encodings_rc[\"attention_mask\"]],\n",
        "    y = [Y_c_train, Y_rephrase_train, np.ones((len(Y_c_train)))],\n",
        "    callbacks = [evaluation_metric_c, checkpoint_c],\n",
        "    batch_size = 128,\n",
        "    shuffle=True,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "NmBCXG6miOxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_c.load_weights('/content/sarcasm-3.007.h5')"
      ],
      "metadata": {
        "id": "oRcLT3D3KOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h6>Test C</h6>"
      ],
      "metadata": {
        "id": "jYMd9Yj1ZoH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_c = pd.read_csv('/content/taskC.En.input.csv')\n",
        "df_test_c.head()"
      ],
      "metadata": {
        "id": "xOY1LUBcWYE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_0_test = df_test_c['text_0']\n",
        "text_1_test = df_test_c['text_1']"
      ],
      "metadata": {
        "id": "Aeq-LPwaWYBV"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_0_test = text_0_test.apply(lambda text: remove_website(text))\n",
        "text_0_test = text_0_test.apply(lambda text: chat_words_conversion(text))\n",
        "text_0_test = text_0_test.apply(lambda text: convert_emojis(text))\n",
        "text_0_test = text_0_test.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Test set completed.......\")"
      ],
      "metadata": {
        "id": "Xk5qAT5XWX-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1_test = text_1_test.apply(lambda text: remove_website(text))\n",
        "text_1_test = text_1_test.apply(lambda text: chat_words_conversion(text))\n",
        "text_1_test = text_1_test.apply(lambda text: convert_emojis(text))\n",
        "text_1_test = text_1_test.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Test set completed.......\")"
      ],
      "metadata": {
        "id": "YYCKNVR_WX8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings_c_0 = tokenizer(list(text_0_test), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(test_encodings_c_0[\"input_ids\"]))\n",
        "\n",
        "test_encodings_c_1 = tokenizer(list(text_1_test), max_length=70, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "print(np.shape(test_encodings_c_1[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "g_eJRGXCJ5M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_c = model_c.predict([test_encodings_c_0['input_ids'], test_encodings_c_0['attention_mask'], test_encodings_c_1['input_ids'], test_encodings_c_1['attention_mask']])"
      ],
      "metadata": {
        "id": "ZS46GmRfKJ0N"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(pred_c)"
      ],
      "metadata": {
        "id": "b_rufbhiKco_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_0 = np.array(np.round(pred_c[0][:,0])).astype(int)\n",
        "pred_test_1 = np.array(np.round(pred_c[1][:,0])).astype(int)"
      ],
      "metadata": {
        "id": "_h6l8tFFK_L2"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(pred_test_0)"
      ],
      "metadata": {
        "id": "Pl_ub_KWNIj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(pred_test_1)"
      ],
      "metadata": {
        "id": "0D0XHA1hLVs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_answer = np.where(pred_test_0 == 0, 1, 0)\n",
        "# Use one of pred_test_0 or pred_test_1\n",
        "final_answer = pred_test_1\n",
        "Counter(final_answer)"
      ],
      "metadata": {
        "id": "6Je_Yr31ND8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('answer.txt', 'w') as outf:\n",
        "  outf.write('task_c_en\\n')\n",
        "  for i in range(0, len(test_encodings_c_0['input_ids'])):\n",
        "    outf.write(str(pred_test_1[i]) + '\\n')"
      ],
      "metadata": {
        "id": "Gj0oJrCRLhUU"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zipfile.ZipFile('sarcasm_electra_c.zip', mode='w').write(\"answer.txt\")"
      ],
      "metadata": {
        "id": "WS-TpcblM5ef"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q_Lzdft8PYPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "iSarcasmEval github",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}